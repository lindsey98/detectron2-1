{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python3\n",
    "\"\"\"Fine-tune a BiT model on some downstream dataset.\"\"\"\n",
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "from os.path import join as pjoin    # pylint: disable=g-importing-member\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "import bit_pytorch.fewshot as fs\n",
    "import bit_pytorch.lbtoolbox as lb\n",
    "import bit_pytorch.models as models\n",
    "\n",
    "import bit_common\n",
    "import bit_hyperrule\n",
    "\n",
    "from .dataloader import GetLoader\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/l/liny/anaconda3/envs/mypy37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/l/liny/anaconda3/envs/mypy37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/l/liny/anaconda3/envs/mypy37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/l/liny/anaconda3/envs/mypy37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/l/liny/anaconda3/envs/mypy37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/l/liny/anaconda3/envs/mypy37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "def topk(output, target, ks=(1,)):\n",
    "    \"\"\"Returns one boolean vector for each k, whether the target is within the output's top-k.\"\"\"\n",
    "    _, pred = output.topk(max(ks), 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return [correct[:k].max(0)[0] for k in ks]\n",
    "\n",
    "\n",
    "def recycle(iterable):\n",
    "    \"\"\"Variant of itertools.cycle that does not save iterates.\"\"\"\n",
    "    while True:\n",
    "        for i in iterable:\n",
    "            yield i\n",
    "\n",
    "\n",
    "def mktrainval(args, logger):\n",
    "    \"\"\"Returns train and validation datasets.\"\"\"\n",
    "    precrop, crop = bit_hyperrule.get_resolution_from_dataset(args.dataset)\n",
    "    train_tx = tv.transforms.Compose([\n",
    "            tv.transforms.Resize((precrop, precrop)),\n",
    "            tv.transforms.RandomCrop((crop, crop)),\n",
    "            tv.transforms.RandomHorizontalFlip(),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    val_tx = tv.transforms.Compose([\n",
    "            tv.transforms.Resize((crop, crop)),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    if args.dataset == \"cifar10\":\n",
    "        train_set = tv.datasets.CIFAR10(args.datadir, transform=train_tx, train=True, download=True)\n",
    "        valid_set = tv.datasets.CIFAR10(args.datadir, transform=val_tx, train=False, download=True)\n",
    "    elif args.dataset == \"cifar100\":\n",
    "        train_set = tv.datasets.CIFAR100(args.datadir, transform=train_tx, train=True, download=True)\n",
    "        valid_set = tv.datasets.CIFAR100(args.datadir, transform=val_tx, train=False, download=True)\n",
    "    elif args.dataset == \"imagenet2012\":\n",
    "        train_set = tv.datasets.ImageFolder(pjoin(args.datadir, \"train\"), train_tx)\n",
    "        valid_set = tv.datasets.ImageFolder(pjoin(args.datadir, \"val\"), val_tx)\n",
    "    # TODO: Define custom dataloading logic here for custom datasets\n",
    "    elif args.dataset == \"logo_2k\":\n",
    "        train_set = GetLoader(data_root='logo2k/Logo-2K+',\n",
    "                                data_list='logo2k/train.txt',\n",
    "                                label_dict='logo2k/logo2k_labeldict.pkl',\n",
    "                                transform=train_tx)\n",
    "        \n",
    "        valid_set = GetLoader(data_root='logo2k/Logo-2K+',\n",
    "                              data_list='logo2k/test.txt',\n",
    "                              label_dict='logo2k/logo2k_labeldict.pkl',\n",
    "                              transform=val_tx)\n",
    "        \n",
    "    elif args.dataset == \"targetlist\":\n",
    "        train_set = GetLoader(data_root='../phishpedia/expand_targetlist',\n",
    "                                            data_list='train_targets.txt',\n",
    "                                            label_dict='target_dict.json',\n",
    "                                            transform=train_tx)\n",
    "        \n",
    "        valid_set = GetLoader(data_root='../phishpedia/expand_targetlist',\n",
    "                              data_list='test_targets.txt',\n",
    "                              label_dict='target_dict.json',\n",
    "                              transform=val_tx)\n",
    "\n",
    "    logger.info(\"Using a training set with {} images.\".format(len(train_set)))\n",
    "    logger.info(\"Using a validation set with {} images.\".format(len(valid_set)))\n",
    "    logger.info(\"Num of classes: {}\".format(len(valid_set.classes)))\n",
    "\n",
    "    micro_batch_size = args.batch // args.batch_split\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "            valid_set, batch_size=micro_batch_size, shuffle=False,\n",
    "            num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    if micro_batch_size <= len(train_set):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                train_set, batch_size=micro_batch_size, shuffle=True,\n",
    "                num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    else:\n",
    "        # In the few-shot cases, the total dataset size might be smaller than the batch-size.\n",
    "        # In these cases, the default sampler doesn't repeat, so we need to make it do that\n",
    "        # if we want to match the behaviour from the paper.\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                train_set, batch_size=micro_batch_size, num_workers=args.workers, pin_memory=True,\n",
    "                sampler=torch.utils.data.RandomSampler(train_set, replacement=True, num_samples=micro_batch_size))\n",
    "\n",
    "    return train_set, valid_set, train_loader, valid_loader\n",
    "\n",
    "\n",
    "def run_eval(model, data_loader, device, chrono, logger, step):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    logger.info(\"Running validation...\")\n",
    "    logger.flush()\n",
    "\n",
    "    all_c, all_top1, all_top5 = [], [], []\n",
    "    end = time.time()\n",
    "    for b, (x, y) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            # measure data loading time\n",
    "            chrono._done(\"eval load\", time.time() - end)\n",
    "\n",
    "            # compute output, measure accuracy and record loss.\n",
    "            with chrono.measure(\"eval fprop\"):\n",
    "                logits = model(x)\n",
    "                c = torch.nn.CrossEntropyLoss(reduction='none')(logits, y)\n",
    "                top1, top5 = topk(logits, y, ks=(1, 5))\n",
    "                all_c.extend(c.cpu())    # Also ensures a sync point.\n",
    "                all_top1.extend(top1.cpu())\n",
    "                all_top5.extend(top5.cpu())\n",
    "\n",
    "        # measure elapsed time\n",
    "        end = time.time()\n",
    "\n",
    "    model.train()\n",
    "    logger.info(\"Validation@{} loss {:.5f}, \".format(step, np.mean(all_c)))\n",
    "    logger.info(\"top1 {:.2%}, \".format(np.mean(all_top1)))\n",
    "    logger.info(\"top5 {:.2%}\".format(np.mean(all_top5)))\n",
    "    logger.flush()\n",
    "    return all_c, all_top1, all_top5\n",
    "\n",
    "\n",
    "def mixup_data(x, y, l):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "    indices = torch.randperm(x.shape[0]).to(x.device)\n",
    "\n",
    "    mixed_x = l * x + (1 - l) * x[indices]\n",
    "    y_a, y_b = y, y[indices]\n",
    "    return mixed_x, y_a, y_b\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, l):\n",
    "    return l * criterion(pred, y_a) + (1 - l) * criterion(pred, y_b)\n",
    "\n",
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "\n",
    "def main(args):\n",
    "    logger = bit_common.setup_logger(args)\n",
    "\n",
    "    # Lets cuDNN benchmark conv implementations and choose the fastest.\n",
    "    # Only good if sizes stay the same within the main loop!\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(\"Going to train on {}\".format(device))\n",
    "\n",
    "    train_set, valid_set, train_loader, valid_loader = mktrainval(args, logger)\n",
    "\n",
    "    logger.info(\"Loading model from {}.npz\".format(args.model))\n",
    "    model = models.KNOWN_MODELS[args.model](head_size=len(valid_set.classes), zero_head=True)\n",
    "    model.load_from(np.load(\"{}.npz\".format(args.model)))\n",
    "\n",
    "    logger.info(\"Moving model onto all GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    # Note: no weight-decay!\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=args.base_lr, momentum=0.9)\n",
    "    \n",
    "    # Optionally resume from a checkpoint.\n",
    "    # Load it to CPU first as we'll move the model to GPU later.\n",
    "    # This way, we save a little bit of GPU memory when loading.\n",
    "    step = 0\n",
    "\n",
    "    # If pretrained weights are specified\n",
    "    if args.weights_path:\n",
    "        logger.info(\"Loading weights from {}\".format(args.weights_path))\n",
    "        checkpoint = torch.load(args.weights_path, map_location=\"cpu\")\n",
    "        # New task might have different classes; remove the pretrained classifier weights\n",
    "        del checkpoint['model']['module.head.conv.weight']\n",
    "        del checkpoint['model']['module.head.conv.bias']\n",
    "        model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "\n",
    "    # Resume fine-tuning if we find a saved model.\n",
    "    savename = pjoin(args.logdir, args.name, \"bit.pth.tar\")\n",
    "    try:\n",
    "        logger.info(\"Model will be saved in '{}'\".format(savename))\n",
    "        checkpoint = torch.load(savename, map_location=\"cpu\")\n",
    "        logger.info(\"Found saved model to resume from at '{}'\".format(savename))\n",
    "\n",
    "        step = checkpoint[\"step\"]\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        optim.load_state_dict(checkpoint[\"optim\"])\n",
    "        logger.info(\"Resumed at step {}\".format(step))\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.info(\"Fine-tuning from BiT\")\n",
    "\n",
    "    # Send to GPU\n",
    "    model = model.to(device)\n",
    "    optimizer_to(optim,device)\n",
    "    optim.zero_grad()\n",
    "\n",
    "    model.train()\n",
    "    mixup = bit_hyperrule.get_mixup(len(train_set))\n",
    "    cri = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    logger.info(\"Starting training!\")\n",
    "    chrono = lb.Chrono()\n",
    "\n",
    "    mixup_l = np.random.beta(mixup, mixup) if mixup > 0 else 1\n",
    "    end = time.time()\n",
    "\n",
    "    with lb.Uninterrupt() as u:\n",
    "        for x, y in recycle(train_loader):\n",
    "            # measure data loading time, which is spent in the `for` statement.\n",
    "            chrono._done(\"load\", time.time() - end)\n",
    "\n",
    "            if u.interrupted:\n",
    "                break\n",
    "\n",
    "            # Schedule sending to GPU(s)\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            # Update learning-rate, including stop training if over.\n",
    "            lr = bit_hyperrule.get_lr(step, len(train_set), args.base_lr)\n",
    "            if lr is None:\n",
    "                break\n",
    "            for param_group in optim.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "            if mixup > 0.0:\n",
    "                x, y_a, y_b = mixup_data(x, y, mixup_l)\n",
    "\n",
    "            # compute output\n",
    "            logits = model(x)\n",
    "            if mixup > 0.0:\n",
    "                c = mixup_criterion(cri, logits, y_a, y_b, mixup_l)\n",
    "            else:\n",
    "                c = cri(logits, y)\n",
    "            c_num = float(c.data.cpu().numpy())    # Also ensures a sync point.\n",
    "\n",
    "            # Accumulate grads\n",
    "            (c / args.batch_split).backward()\n",
    "\n",
    "            logger.info(\"[step {}]: loss={:.5f} (lr={:.1e})\".format(step, c_num, lr))    \n",
    "            logger.flush()\n",
    "\n",
    "            # Update params\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            step += 1\n",
    "\n",
    "            # Sample new mixup ratio for next batch\n",
    "            mixup_l = np.random.beta(mixup, mixup) if mixup > 0 else 1\n",
    "            \n",
    "            # Save model\n",
    "            end = time.time()\n",
    "            if step % 50 == 0:\n",
    "                torch.save({\n",
    "                    \"step\": step,\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optim\" : optim.state_dict(),\n",
    "                }, savename)\n",
    "\n",
    "        # Final eval at end of training.\n",
    "        run_eval(model, valid_loader, device, chrono, logger, step='end')\n",
    "        torch.save({\n",
    "            \"step\": step,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optim\" : optim.state_dict(),\n",
    "        }, savename)\n",
    "\n",
    "\n",
    "    logger.info(\"Timings:\\n{}\".format(chrono))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = bit_common.argparser(models.KNOWN_MODELS.keys())\n",
    "    parser.add_argument(\"--workers\", type=int, default=8,\n",
    "                                            help=\"Number of background threads used to load data.\")\n",
    "    parser.add_argument(\"--no-save\", dest=\"save\", action=\"store_false\")\n",
    "    main(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
