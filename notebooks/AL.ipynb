{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2_1\n",
    "import time\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from pathlib import Path\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "import cv2\n",
    "from itertools import product\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "from detectron2.modeling import build_model\n",
    "import numpy as np\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
    "from detectron2.structures import Instances\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.utils.events import EventStorage\n",
    "\n",
    "from detectron2.data.samplers import InferenceSampler, TrainingSampler\n",
    "from detectron2.data.build import get_detection_dataset_dicts\n",
    "from detectron2.data.common import DatasetFromList, MapDataset\n",
    "from detectron2.data import build_batch_data_loader, DatasetMapper\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path('outputs')/'coco-detection'\n",
    "rcnn_weights_path = model_dir/'model_final_280758.pkl'\n",
    "rcnn_cfg_path = Path('configs/COCO-Detection')/'faster_rcnn_R_50_FPN_3x_test.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2_1.configs import get_cfg\n",
    "\n",
    "def setup(config_file, rcnn_weights_path):\n",
    "    \"\"\"\n",
    "    Create configs and perform basic setups.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the configurations\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_file)\n",
    "    \n",
    "    # Ensure it uses appropriate names and architecture  \n",
    "    cfg.AL.OBJECT_SCORING = 'entropy'\n",
    "    cfg.MODEL.ROI_HEADS.NAME = 'ROIHeadsAL'\n",
    "    cfg.MODEL.META_ARCHITECTURE = 'ActiveLearningRCNN'\n",
    "    cfg.MODEL.WEIGHTS = str(rcnn_weights_path)\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05 # lower this threshold to get more boxes\n",
    "\n",
    "    cfg.freeze()\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = setup(rcnn_cfg_path, rcnn_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'MODE': 'object', 'OBJECT_SCORING': 'entropy', 'IMAGE_SCORE_AGGREGATION': 'avg', 'PERTURBATION': CfgNode({'VERSION': 1, 'ALPHAS': [0.08, 0.12], 'BETAS': [0.04, 0.16], 'RANDOM': False, 'LAMBDA': 1.0}), 'DATASET': CfgNode({'NAME': '', 'IMG_ROOT': '', 'ANNO_PATH': '', 'CACHE_DIR': 'al_datasets', 'NAME_PREFIX': 'r', 'BUDGET_STYLE': 'object', 'IMAGE_BUDGET': 20, 'OBJECT_BUDGET': 2000, 'BUDGET_ALLOCATION': 'linear', 'SAMPLE_METHOD': 'top'}), 'OBJECT_FUSION': CfgNode({'OVERLAPPING_METRIC': 'iou', 'OVERLAPPING_TH': 0.25, 'SELECTION_METHOD': 'top', 'REMOVE_DUPLICATES': True, 'REMOVE_DUPLICATES_TH': 0.15, 'RECOVER_MISSING_OBJECTS': True, 'INITIAL_RATIO': 0.85, 'LAST_RATIO': 0.25, 'DECAY': 'linear', 'PRESELECTION_RAIO': 1.5, 'ENDSELECTION_RAIO': 1.25, 'SELECTION_RAIO_DECAY': 'linear', 'RECOVER_ALMOST_CORRECT_PRED': True, 'BUDGET_ETA': 0.2}), 'TRAINING': CfgNode({'ROUNDS': 5, 'EPOCHS_PER_ROUND_INITIAL': 500, 'EPOCHS_PER_ROUND_DECAY': 'linear', 'EPOCHS_PER_ROUND_LAST': 50})})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cfg = get_cfg()\n",
    "# add_al_config(cfg)\n",
    "# cfg.merge_from_file(rcnn_cfg_path)\n",
    "# cfg.MODEL.WEIGHTS = str(rcnn_weights_path)\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05 # lower this threshold to get more boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2_1.modelling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(cfg)\n",
    "model.eval()\n",
    "\n",
    "# Build_model will not load weights, have to load weights explicitly\n",
    "checkpointer = DetectionCheckpointer(model)  \n",
    "checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "# Metadata of dataset\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
    "\n",
    "# Testtime augmentation is random resize\n",
    "aug = T.ResizeShortestEdge([cfg.INPUT.MIN_SIZE_TEST, \n",
    "                            cfg.INPUT.MIN_SIZE_TEST], \n",
    "                            cfg.INPUT.MAX_SIZE_TEST) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rewrite data loader because we want data loader with batch size > 1 and no shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dicts = get_detection_dataset_dicts(\n",
    "#     cfg.DATASETS.TRAIN,\n",
    "#     filter_empty=cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS,\n",
    "#     min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
    "#     if cfg.MODEL.KEYPOINT_ON\n",
    "#     else 0,\n",
    "#     proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN if cfg.MODEL.LOAD_PROPOSALS else None,\n",
    "# )\n",
    "\n",
    "# dataset = DatasetFromList(dataset_dicts)\n",
    "# mapper = DatasetMapper(cfg, False) # set training = True? to enable gradients calculation\n",
    "# dataset = MapDataset(dataset, mapper)\n",
    "\n",
    "# # sampler = InferenceSampler(len(dataset))\n",
    "# # batch_sampler = torch.utils.data.sampler.BatchSampler(sampler, 4, drop_last=False) # reduce batch size to 4\n",
    "\n",
    "# # def trivial_batch_collator(batch):\n",
    "# #     \"\"\"\n",
    "# #     A batch collator that does nothing.\n",
    "# #     \"\"\"\n",
    "# #     return batch\n",
    "\n",
    "# # data_loader = torch.utils.data.DataLoader(\n",
    "# #     dataset,\n",
    "# #     num_workers=cfg.DATALOADER.NUM_WORKERS,\n",
    "# #     batch_sampler=batch_sampler,\n",
    "# #     collate_fn=trivial_batch_collator,\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Training data loader \n",
    "\n",
    "# sampler = TrainingSampler(len(dataset), shuffle=False) # no shuffle\n",
    "# # this will be an infinite data loader \n",
    "# data_loader = build_batch_data_loader(\n",
    "#         dataset,\n",
    "#         sampler,\n",
    "#         4, # batch size equals 1\n",
    "#         aspect_ratio_grouping=True,\n",
    "#         num_workers=cfg.DATALOADER.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import build_detection_test_loader\n",
    "dataset_mapper = DatasetMapper(cfg, False) # set training = True? to enable gradients calculation\n",
    "data_loader= build_detection_test_loader(\n",
    "    cfg, cfg.DATASETS.TEST[0], mapper=dataset_mapper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Enter training mode ###\n",
    "# model.train()\n",
    "# with EventStorage() as storage: # during training, we have to add this to suppress error\n",
    "#     loss_dict = model(data)\n",
    "    \n",
    "# model.zero_grad() # zero out gradients first\n",
    "# losses = sum(loss_dict.values())\n",
    "\n",
    "# losses.backward()\n",
    "# gradients_last = model.roi_heads.box_predictor.cls_score.weight.grad # get gradients of last layer\n",
    "\n",
    "### Enter evaluation mode ###\n",
    "# model.eval()\n",
    "# outputs = model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(data_loader):\n",
    "    break\n",
    "    if i >= len(dataset):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_for_proposals(model, features, proposals, return_grad=False):\n",
    "    \n",
    "    box2boxtransform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
    "    gradients_vec = []\n",
    "    \n",
    "#     if not return_grad:\n",
    "    with torch.no_grad():\n",
    "        features = [features[f] for f in model.roi_heads.box_in_features]\n",
    "        box_features = model.roi_heads.box_pooler(features,\n",
    "                            [x if isinstance(x, Boxes) \\\n",
    "                                else x.proposal_boxes for x in proposals])\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "        pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(box_features)\n",
    "        del box_features\n",
    "\n",
    "#     else:\n",
    "#     with torch.enable_grad():\n",
    "#         features = [features[f] for f in model.roi_heads.box_in_features]\n",
    "#         box_features = model.roi_heads.box_pooler(features,\n",
    "#                             [x if isinstance(x, Boxes) \\\n",
    "#                                 else x.proposal_boxes for x in proposals])\n",
    "#         box_features = model.roi_heads.box_head(box_features)\n",
    "#         pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(box_features)\n",
    "#         _, pred_class_indices = torch.max(pred_class_logits, -1)        \n",
    "#         print(pred_class_indices.shape)\n",
    "        \n",
    "#         m = torch.nn.LogSoftmax(dim=-1)\n",
    "#         loss = torch.nn.NLLLoss()\n",
    "#         target = pred_class_indices\n",
    "#         tot_loss = loss(m(pred_class_logits), target)\n",
    "#         print(tot_loss)\n",
    "#         tot_loss.backward()\n",
    "#         # only get gradients w.r.t. predicted class for computational feasibility\n",
    "#         gradients = model.roi_heads.box_predictor.cls_score.weight.grad.detach().cpu()\n",
    "        \n",
    "#         _, pred_class_indices = torch.max(pred_class_logits, -1)            \n",
    "\n",
    "#         for j, ind in enumerate(pred_class_indices):\n",
    "#             model.zero_grad()\n",
    "#             pred_class_logits[j, ind].backward(retain_graph=True)\n",
    "#             gradients_j = model.roi_heads.box_predictor.cls_score.weight.grad[ind].detach().cpu()\n",
    "#             gradients_vec.append(gradients_j)\n",
    "#         del box_features\n",
    "\n",
    "    outputs = FastRCNNOutputs(\n",
    "        box2boxtransform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        proposals,\n",
    "        cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA)\n",
    "    \n",
    "    return outputs\n",
    "#     return outputs, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(data)\n",
    "# outputs.backward()\n",
    "# print(model.roi_heads.box_predictor.cls_score.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess image\n",
    "images = model.preprocess_image(data)\n",
    "\n",
    "# Get features\n",
    "features = model.backbone(images.tensor)\n",
    "\n",
    "# Get bounding box proposals\n",
    "proposals, _ = model.proposal_generator(images, features, None)\n",
    "\n",
    "# Get predicted boxes and do roi_head postprocessing(filter by scores, NMS, select topk)\n",
    "outputs = estimate_for_proposals(model, features, proposals, return_grad=True)\n",
    "pred_boxes = outputs.predict_boxes()\n",
    "pred_probs = outputs.predict_probs()\n",
    "\n",
    "# Perform postprocessing\n",
    "cur_detections, filtered_indices = outputs.inference(cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST,\n",
    "                                                     cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST, \n",
    "                                                     cfg.TEST.DETECTIONS_PER_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avgpool over channels\n",
    "features_heatmaps = [torch.mean(features[k], dim=1) for k in features.keys()]\n",
    "\n",
    "# Adaptive pooling 2D --> Tensor(shape 5x8x8)]\n",
    "pooled_heatmaps = torch.cat([F.adaptive_avg_pool2d(f_map[None, ...], output_size=(8, 8)) \\\n",
    "                             .view(1, 8, 8).detach().cpu() \\\n",
    "                             for f_map in features_heatmaps], dim=0)\n",
    "pooled_heatmaps = pooled_heatmaps.view(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_heatmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_refs = torch.rand((5, 320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 81])\n",
      "tensor([0.0003, 0.0005, 0.0010, 0.0034, 0.0035, 0.0038, 0.0040, 0.0050, 0.0045,\n",
      "        0.0048, 0.0053, 0.0053, 0.0056, 0.0071, 0.0069, 0.0090, 0.0086, 0.0124,\n",
      "        0.0094, 0.0111, 0.0161, 0.0136, 0.0087, 0.0091, 0.0091, 0.0104, 0.0163,\n",
      "        0.0076, 0.0122, 0.0077, 0.0163, 0.0091, 0.0072, 0.0134, 0.0069, 0.0076,\n",
      "        0.0091, 0.0136, 0.0059, 0.0060, 0.0126, 0.0068, 0.0057, 0.0058, 0.0095,\n",
      "        0.0051, 0.0127, 0.0051, 0.0050, 0.0056, 0.0145, 0.0095, 0.0048, 0.0058,\n",
      "        0.0067, 0.0047, 0.0049, 0.0047, 0.0040, 0.0068, 0.0082, 0.0049, 0.0053,\n",
      "        0.0044, 0.0065, 0.0043, 0.0117, 0.0052, 0.0041, 0.0039, 0.0048, 0.0079,\n",
      "        0.0084, 0.0053, 0.0041, 0.0059, 0.0035, 0.0043, 0.0047, 0.0261, 0.0066,\n",
      "        0.0041, 0.0063, 0.0029, 0.0051, 0.0041, 0.0090, 0.0090, 0.0031, 0.0053],\n",
      "       device='cuda:0')\n",
      "tensor([0.0003, 0.0005, 0.0010, 0.0034, 0.0035, 0.0038, 0.0040, 0.0050, 0.0045,\n",
      "        0.0048, 0.0053, 0.0053, 0.0056, 0.0071, 0.0069, 0.0090, 0.0086, 0.0124,\n",
      "        0.0094, 0.0111, 0.0161, 0.0136, 0.0087, 0.0091, 0.0091, 0.0104, 0.0163,\n",
      "        0.0076, 0.0122, 0.0077, 0.0163, 0.0091, 0.0072, 0.0134, 0.0069, 0.0076,\n",
      "        0.0091, 0.0136, 0.0059, 0.0060, 0.0126, 0.0068, 0.0057, 0.0058, 0.0095,\n",
      "        0.0051, 0.0127, 0.0051, 0.0050, 0.0056, 0.0145, 0.0095, 0.0048, 0.0058,\n",
      "        0.0067, 0.0047, 0.0049, 0.0047, 0.0040, 0.0068, 0.0082, 0.0049, 0.0053,\n",
      "        0.0044, 0.0065, 0.0043, 0.0117, 0.0052, 0.0041, 0.0039, 0.0048, 0.0079,\n",
      "        0.0084, 0.0053, 0.0041, 0.0059, 0.0035, 0.0043, 0.0047, 0.0261, 0.0066,\n",
      "        0.0041, 0.0063, 0.0029, 0.0051, 0.0041, 0.0090, 0.0090, 0.0031, 0.0053])\n"
     ]
    }
   ],
   "source": [
    "outputs = model.forward_al(data, feature_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0003, 0.0005, 0.0010, 0.0034, 0.0035, 0.0038, 0.0040, 0.0050, 0.0045,\n",
       "        0.0048, 0.0053, 0.0053, 0.0056, 0.0071, 0.0069, 0.0090, 0.0086, 0.0124,\n",
       "        0.0094, 0.0111, 0.0161, 0.0136, 0.0087, 0.0091, 0.0091, 0.0104, 0.0163,\n",
       "        0.0076, 0.0122, 0.0077, 0.0163, 0.0091, 0.0072, 0.0134, 0.0069, 0.0076,\n",
       "        0.0091, 0.0136, 0.0059, 0.0060, 0.0126, 0.0068, 0.0057, 0.0058, 0.0095,\n",
       "        0.0051, 0.0127, 0.0051, 0.0050, 0.0056, 0.0145, 0.0095, 0.0048, 0.0058,\n",
       "        0.0067, 0.0047, 0.0049, 0.0047, 0.0040, 0.0068, 0.0082, 0.0049, 0.0053,\n",
       "        0.0044, 0.0065, 0.0043, 0.0117, 0.0052, 0.0041, 0.0039, 0.0048, 0.0079,\n",
       "        0.0084, 0.0053, 0.0041, 0.0059, 0.0035, 0.0043, 0.0047, 0.0261, 0.0066,\n",
       "        0.0041, 0.0063, 0.0029, 0.0051, 0.0041, 0.0090, 0.0090, 0.0031, 0.0053])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]['instances'].scores_al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 200, 272])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['p2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 81])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = torch.stack(pred_probs)\n",
    "filter_prob = [pred_probs[i, filtered_index, :] for i, filtered_index in enumerate(filtered_indices)]\n",
    "\n",
    "# compute entropy \n",
    "entropy_scores = [torch.sum(-probs*torch.log2(probs), dim=-1) for idx, probs in enumerate(filter_prob)]\n",
    "\n",
    "for cur_detection, object_score in zip(cur_detections, entropy_scores):\n",
    "    cur_detection.scores_al = object_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'TRAIN': ('coco_2017_train',), 'PROPOSAL_FILES_TRAIN': (), 'PRECOMPUTED_PROPOSAL_TOPK_TRAIN': 2000, 'TEST': ('coco_2017_val',), 'PROPOSAL_FILES_TEST': (), 'PRECOMPUTED_PROPOSAL_TOPK_TEST': 1000})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataset 'coco_2017_train_subset1' is already registered!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-7a7c8fa0356b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_subset1_coco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"datasets/coco/annotations/instances_train2017_subset1.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_subset2_coco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"datasets/coco/annotations/instances_train2017_subset2.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mregister_coco_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coco_2017_train_subset1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_subset1_coco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mregister_coco_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coco_2017_train_subset2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_subset2_coco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypy37/lib/python3.7/site-packages/detectron2/data/datasets/register_coco.py\u001b[0m in \u001b[0;36mregister_coco_instances\u001b[0;34m(name, metadata, json_file, image_root)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# 1. register a function which returns dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_coco_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# 2. Optionally, add metadata about this dataset,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypy37/lib/python3.7/site-packages/detectron2/data/catalog.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(name, func)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You must register a function with `DatasetCatalog.register`!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         assert name not in DatasetCatalog._REGISTERED, \"Dataset '{}' is already registered!\".format(\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         )\n\u001b[1;32m     42\u001b[0m         \u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REGISTERED\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Dataset 'coco_2017_train_subset1' is already registered!"
     ]
    }
   ],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "# Define dataset paths\n",
    "data_dir = Path(\"datasets/coco/train2017/\")\n",
    "train_subset1_coco = \"datasets/coco/annotations/instances_train2017_subset1.json\"\n",
    "train_subset2_coco = \"datasets/coco/annotations/instances_train2017_subset2.json\"\n",
    "register_coco_instances(\"coco_2017_train_subset1\", {}, train_subset1_coco, data_dir)\n",
    "register_coco_instances(\"coco_2017_train_subset2\", {}, train_subset2_coco, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path('outputs')/'coco-detection'\n",
    "rcnn_weights_path = model_dir/'model_final_280758.pkl'\n",
    "rcnn_cfg_path = Path('configs/COCO-Detection')/'faster_rcnn_R_50_FPN_3x.yaml'\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(rcnn_cfg_path)\n",
    "cfg.MODEL.WEIGHTS = str(rcnn_weights_path)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05 # lower this threshold to get more boxes\n",
    "cfg.DATASETS.TRAIN = ('coco_2017_train_subset1',)\n",
    "cfg.DATASETS.TEST = ('coco_2017_train_subset2',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_entropy_scores(p):\n",
    "    # use entropy\n",
    "    entropy = torch.sum(-(p * torch.log(p)), dim=-1)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "######### For perturbed box #######################################################\n",
    "def iou(area1, area2, inter_area):\n",
    "    return inter_area / (area1 + area2 - inter_area)\n",
    "\n",
    "def elementwise_intersect_area(boxes1, boxes2):\n",
    "\n",
    "    # Modified based on \n",
    "    # https://detectron2.readthedocs.io/_modules/detectron2/structures/boxes.html#pairwise_iou\n",
    "    boxes1, boxes2 = boxes1.tensor, boxes2.tensor\n",
    "\n",
    "    width_height = torch.min(boxes1[:, 2:], boxes2[:, 2:]) - torch.max(boxes1[:, :2], boxes2[:, :2])  # [N,M,2]\n",
    "\n",
    "    width_height.clamp_(min=0)  # [N,2]\n",
    "    inter = width_height.prod(dim=-1)  # [N]\n",
    "    del width_height\n",
    "    return inter\n",
    "\n",
    "def elementwise_iou(boxes1, boxes2):\n",
    "    \n",
    "    area1, area2 = boxes1.area(), boxes2.area()\n",
    "    inter_area = elementwise_intersect_area(boxes1, boxes2)\n",
    "\n",
    "    scores = torch.where(\n",
    "            inter_area > 0,\n",
    "            iou(area1, area2, inter_area),\n",
    "            torch.zeros(1, dtype=inter_area.dtype, device=inter_area.device),\n",
    "        )\n",
    "    return scores\n",
    "\n",
    "def calculate_iou_scores(perturbed_box, raw_det, num_shifts, num_bbox_reg_classes):\n",
    "    reshaped_boxes = perturbed_box.reshape(-1, num_bbox_reg_classes, 4)\n",
    "    cat_ids = raw_det.pred_classes.repeat_interleave(num_shifts, dim=0)\n",
    "    perturbed_boxes = Boxes(torch.stack([reshaped_boxes[row_id, cat_id] for row_id, cat_id in enumerate(cat_ids)]))\n",
    "    raw_boxes = Boxes(raw_det.pred_boxes.tensor.repeat_interleave(num_shifts, dim=0))\n",
    "    ious = elementwise_iou(raw_boxes, perturbed_boxes)\n",
    "    # aggregate the statistics for each prediction\n",
    "    iou_scores = torch.Tensor([scores.mean() for scores in ious.split(num_shifts)])\n",
    "    return iou_scores\n",
    "\n",
    "######### For perturbed box #######################################################\n",
    "def calculate_ce_scores(p, q, num_shifts):\n",
    "    # use crossentropy for calculation diff\n",
    "    diff = - (p * torch.log(q)).mean(dim=-1)\n",
    "    # aggregate the statistics for each prediction\n",
    "    diff = torch.Tensor([scores.mean() for scores in diff.split(num_shifts)]) \n",
    "\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearning():\n",
    "    \n",
    "    def __init__(self, cfg, score_func):\n",
    "        \n",
    "        # Initialize model\n",
    "        self.cfg = cfg\n",
    "        self.model = build_model(cfg)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Build_model will not load weights, have to load weights explicitly\n",
    "        checkpointer = DetectionCheckpointer(self.model)  \n",
    "        checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "        # Metadata of dataset\n",
    "        self.metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
    "        \n",
    "        # Testtime augmentation is random resize\n",
    "        self.aug = T.ResizeShortestEdge([cfg.INPUT.MIN_SIZE_TEST, \n",
    "                                         cfg.INPUT.MIN_SIZE_TEST], \n",
    "                                         cfg.INPUT.MAX_SIZE_TEST) \n",
    "        # Perturbation matrix for xywh\n",
    "        self.num_shifts, self.shift_matrix = self._create_translations()   \n",
    "        \n",
    "        # scoring function\n",
    "        self.score_func = score_func\n",
    "        \n",
    "        # data loader\n",
    "        self.data_loader, self.len_data = self._load_dataset()\n",
    "    \n",
    "        dataset_mapper = DatasetMapper(cfg, False) # set training = False\n",
    "        self.data_loader= build_detection_test_loader(\n",
    "            cfg, cfg.DATASETS.TEST[0], mapper=dataset_mapper\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _estimate_for_proposals(self, features, proposals):\n",
    "    \n",
    "        box2boxtransform = Box2BoxTransform(weights=self.cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = [features[f] for f in self.model.roi_heads.box_in_features]\n",
    "            box_features = self.model.roi_heads.box_pooler(features,\n",
    "                                [x if isinstance(x, Boxes) \\\n",
    "                                    else x.proposal_boxes for x in proposals])\n",
    "            box_features = self.model.roi_heads.box_head(box_features)\n",
    "            pred_class_logits, pred_proposal_deltas = self.model.roi_heads.box_predictor(box_features)\n",
    "            del box_features\n",
    "\n",
    "        outputs = FastRCNNOutputs(\n",
    "            box2boxtransform,\n",
    "            pred_class_logits,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            self.cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA)\n",
    "\n",
    "        return outputs\n",
    "        \n",
    "    @staticmethod\n",
    "    def _create_translations():\n",
    "        '''Function to generate perturbation matrix'''\n",
    "        # https://github.com/lolipopshock/Detectron2_AL\n",
    "        def _generate_individual_shift_matrix(alpha, beta):\n",
    "            return torch.Tensor([\n",
    "                    [(1-alpha), 0,       -alpha,    0],\n",
    "                    [0,        (1-beta), 0,         -beta],\n",
    "                    [alpha,     0,       (1+alpha), 0],\n",
    "                    [0,         beta,    0,         (1+beta)],\n",
    "                ])\n",
    "\n",
    "        # Horizontal translation ratio\n",
    "        alphas = [0.08, 0.12] \n",
    "        # Vertical translation ratio\n",
    "        betas  = [0.04, 0.16] \n",
    "\n",
    "        derived_shift = [\n",
    "            [\n",
    "                [alpha, beta], \n",
    "                [alpha, -beta], \n",
    "                [-alpha, beta], \n",
    "                [-alpha, -beta]\n",
    "            ] \n",
    "            for alpha, beta in product(alphas, betas)\n",
    "        ]\n",
    "\n",
    "        matrices = [_generate_individual_shift_matrix(*params) \n",
    "                        for params in sum(derived_shift, [])]\n",
    "        return len(matrices), torch.stack(matrices, dim=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _feature_embed(features):\n",
    "        # Avgpool over channels\n",
    "        features_heatmaps = [torch.mean(features[k], dim=1) for k in features.keys()]\n",
    "        \n",
    "        # Adaptive pooling 2D --> Tensor(shape 5x32x32)]\n",
    "        pooled_heatmaps = torch.cat([F.adaptive_avg_pool2d(f_map[None, ...], output_size=(32, 32)) \\\n",
    "                                     .view(1, 32, 32).detach().cpu() \\\n",
    "                                     for f_map in features_heatmaps], dim=0)\n",
    "        \n",
    "        return pooled_heatmaps\n",
    "    \n",
    "    def _perturb_consistency(self, cur_detections, features, orig_prob):\n",
    "        # Box Perturbations\n",
    "        # Get new proposals\n",
    "        orig_boxes = cur_detections[0].pred_boxes.tensor\n",
    "        new_proposals = Instances(cur_detections[0].image_size,\n",
    "                                  proposal_boxes=Boxes(\n",
    "                                      torch.einsum('bi,ijc->bjc', \n",
    "                                      orig_boxes,\n",
    "                                      self.shift_matrix.to(orig_boxes.device)).permute(0,2,1).reshape(-1,4))) \n",
    "\n",
    "        perturbed_outputs = self._estimate_for_proposals(features, [new_proposals]) \n",
    "        perturbed_probs = perturbed_outputs.predict_probs()[0]\n",
    "        perturbed_boxes = perturbed_outputs.predict_boxes()[0]\n",
    "        num_bbox_reg_classes = perturbed_boxes.shape[1] // 4\n",
    "\n",
    "        p = orig_prob.repeat_interleave(self.num_shifts, dim=0)\n",
    "        q = perturbed_probs\n",
    "\n",
    "        # Compute difference in IoU and Cross-Entropy\n",
    "        diff1 = calculate_iou_scores(perturbed_boxes, cur_detections[0], \n",
    "                                     self.num_shifts, num_bbox_reg_classes)\n",
    "        diff2 = calculate_ce_scores(p, q, self.num_shifts)\n",
    "        box_diff = diff1 + diff2\n",
    "        return box_diff\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        for i, data in enumerate(self.data_loader):\n",
    "                \n",
    "            file_name = data[0]['file_name']\n",
    "            image_id = data[0]['image_id']\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Preprocess images \n",
    "                images = self.model.preprocess_image(data)\n",
    "\n",
    "                # Get features\n",
    "                features = self.model.backbone(images.tensor)\n",
    "\n",
    "                # Get bounding box proposals\n",
    "                proposals, _ = self.model.proposal_generator(images, features, None)\n",
    "\n",
    "                # Get predicted boxes and do roi_head postprocessing(filter by scores, NMS, select topk)\n",
    "                outputs = self._estimate_for_proposals(features, proposals)\n",
    "                pred_boxes = outputs.predict_boxes()[0] # batch size is 1\n",
    "                pred_probs = outputs.predict_probs()[0]\n",
    "                cur_detections, filtered_indices = outputs.inference(self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST,\n",
    "                                               self.cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST, \n",
    "                                               self.cfg.TEST.DETECTIONS_PER_IMAGE)\n",
    "                filter_prob = pred_probs[filtered_indices[0], :]\n",
    "                \n",
    "                if self.score_func == 1:\n",
    "                    ## Entropy\n",
    "                    scores = calculate_entropy_scores(filter_prob)\n",
    "                    cur_detections[0].scores_al = scores  # batch size is 1\n",
    "#                     return cur_detections\n",
    "                    \n",
    "                elif self.score_func == 2:\n",
    "                    ## Perturbed boxes consistency\n",
    "                    scores = self._perturb_consistency(cur_detections, features, filter_prob)\n",
    "                    cur_detections[0].scores_al = scores  # batch size is 1\n",
    "#                     return cur_detections\n",
    "                    \n",
    "                elif self.score_func == 3:\n",
    "                    ## Get embedding of FPN \n",
    "                    embed = self._feature_embed(features).detach().cpu() # per-image basis\n",
    "#                     return cur_detections, embed                \n",
    "            \n",
    "                del images, features, proposals, outputs\n",
    "        \n",
    "    def save_res(self, detections, file_name, image_id, write_path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = ActiveLearning(cfg, score_func=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = al.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 16])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haha[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10).reshape(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1]]),\n",
       " tensor([[2, 3]]),\n",
       " tensor([[4, 5]]),\n",
       " tensor([[6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
