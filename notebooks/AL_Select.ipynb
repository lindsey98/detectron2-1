{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feed into element detector \n",
    "- Feed into credential classifier\n",
    "- Select suspicious data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../example2.png\" style=\"width:2000px;height:350px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\" # use all devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2_1.datasets import WebMapper\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import funcy\n",
    "from IPython.display import clear_output\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from pycocotools import cocoeval, coco\n",
    "from detectron2.data import build_detection_test_loader, MetadataCatalog, DatasetCatalog\n",
    "import numpy as np\n",
    "import tldextract\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credential_classifier.bit_pytorch.models import FCMaxPool\n",
    "from credential_classifier.bit_pytorch.grid_divider import read_img_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credential import *\n",
    "from element_detector import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_cfg, ele_model = element_config(rcnn_weights_path = 'output/website_lr0.001/model_final.pth', \n",
    "                                    rcnn_cfg_path='configs/faster_rcnn_web.yaml')\n",
    "\n",
    "cls_model = credential_config(checkpoint='credential_classifier/FCMax_0.05.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get uncertainty and feature embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49213/49213 [2:21:54<00:00,  5.78it/s]   \n"
     ]
    }
   ],
   "source": [
    "entropy_agg = []\n",
    "margin_agg = []\n",
    "feature_agg = []\n",
    "\n",
    "for file in tqdm(os.listdir('./datasets/AL_pool_imgs/')):\n",
    "    img_path = os.path.join('./datasets/AL_pool_imgs', file)\n",
    "\n",
    "    pred_classes, pred_boxes, pred_scores = element_recognition(img=img_path, model=ele_model)\n",
    "\n",
    "    cls_pred, cls_conf, feature = credential_classifier_al(img_path, pred_boxes, pred_classes, cls_model)\n",
    "    \n",
    "    cls_conf = cls_conf[0].numpy()\n",
    "    \n",
    "    entropy = -np.sum(cls_conf * np.log2(cls_conf), axis=-1).item() # entropy\n",
    "    margin = cls_conf[cls_conf.argsort()[::-1][0]] - cls_conf[cls_conf.argsort()[::-1][1]] # top1-top2\n",
    "    \n",
    "    entropy_agg.append(entropy)\n",
    "    margin_agg.append(margin)\n",
    "    feature_agg.append(feature.detach().cpu().numpy()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_agg = np.asarray(entropy_agg)\n",
    "margin_agg = np.asarray(margin_agg)\n",
    "feature_agg = np.asarray(feature_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_agg = []\n",
    "for feat in feature_agg:\n",
    "    new_feature_agg.append(feat.detach().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_agg = np.asarray(new_feature_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./entropy_al.npy', entropy_agg)\n",
    "np.save('./margin_al.npy', margin_agg)\n",
    "np.save('./feature_al.npy', new_feature_agg)\n",
    "np.save('./al_files.npy', np.asarray(os.listdir('./datasets/AL_pool_imgs/')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_plus(S, feat, N):\n",
    "    '''\n",
    "    Implementation of kmeans++\n",
    "    :param S: (n,) uncertainty scores\n",
    "    :param feat: (n,f) features array\n",
    "    :param N: how many AL to be selected\n",
    "    :return (N,) selected indices\n",
    "    '''\n",
    "    \n",
    "    S = np.asarray(S)\n",
    "    feat = np.asarray(feat)\n",
    "    c_sets = []\n",
    "    \n",
    "    # compute similarity matrix --> convert to cosine distance\n",
    "    feat = F.normalize(torch.from_numpy(feat), dim=1, p=2).numpy()\n",
    "    D = 1 - feat @ feat.T # nxn\n",
    "    D = np.clip(D, a_min=0., a_max=1.) # assure correct range\n",
    "\n",
    "    # randomly find first centroid\n",
    "    c0 = random.sample(range(len(S)), 1)[0]\n",
    "    c_sets.append(c0)\n",
    "    \n",
    "    # for loop until N centroids are found\n",
    "    while len(c_sets) < N:\n",
    "        # create mask so centroids will not be covered\n",
    "        mask = np.ones((len(S),), bool)\n",
    "        mask[np.asarray(c_sets)] = False # mask of shape (n,)\n",
    "\n",
    "        # get distance w.r.t. nearest centroid\n",
    "        D_select = D[mask, :][:, np.asarray(c_sets)]\n",
    "        mind = np.min(D_select, axis=1) # mind of shape (n-c,)\n",
    "\n",
    "        # sample according to distribution\n",
    "        p_sample = mind*S[mask]/np.sum(mind*S[mask]) # p_sample of shape (n-c,)\n",
    "        ci = np.random.choice(np.asarray(range(len(S)))[mask], 1, p=p_sample)[0]\n",
    "        \n",
    "        # add to centroid list\n",
    "        c_sets.append(ci)\n",
    "        if len(c_sets) % 100 == 0:\n",
    "            print(len(c_sets))\n",
    "        \n",
    "    # return n centroids\n",
    "    return c_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n"
     ]
    }
   ],
   "source": [
    "c_sets = kmeans_plus(S=entropy_agg, feat=new_feature_agg, N=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
