{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Dict, List, Tuple, Callable\n",
    "\n",
    "import cv2\n",
    "import detectron2.data.transforms as T\n",
    "import numpy as np\n",
    "import torch\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import CfgNode\n",
    "from detectron2.data import DatasetMapper, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.structures import Boxes, pairwise_iou\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "os.environ['QT_QPA_PLATFORM']='offscreen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing config file...\n"
     ]
    }
   ],
   "source": [
    "from detectron2.config import get_cfg\n",
    "print(\"Preparing config file...\")\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file('../configs/faster_rcnn_bet365.yaml')\n",
    "cfg.MODEL.WEIGHTS = '../rcnn_bet365.pth'\n",
    "\n",
    "# Modify config\n",
    "cfg = cfg.clone()  # cfg can be modified by model\n",
    "# To generate more dense proposals\n",
    "cfg.MODEL.RPN.NMS_THRESH = 0.9\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 3000\n",
    "\n",
    "# Init model\n",
    "model = build_model(cfg)\n",
    "model.eval()\n",
    "\n",
    "# Load weights\n",
    "checkpointer = DetectionCheckpointer(model)\n",
    "checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "# data augmentation\n",
    "aug = T.ResizeShortestEdge(\n",
    "    [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GeneralizedRCNN.preprocess_image of GeneralizedRCNN(\n",
       "  (backbone): FPN(\n",
       "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): ResNet(\n",
       "      (stem): BasicStem(\n",
       "        (conv1): Conv2d(\n",
       "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (res2): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res3): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res4): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res5): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proposal_generator): RPN(\n",
       "    (rpn_head): StandardRPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (anchor_generator): DefaultAnchorGenerator(\n",
       "      (cell_anchors): BufferList()\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): StandardROIHeads(\n",
       "    (box_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNOutputLayers(\n",
       "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.preprocess_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save transformed ground-truth result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetMapper, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2_1.datasets import BenignMapper\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_mapper = BenignMapper(cfg, is_train=False)\n",
    "data_loader = build_detection_test_loader(\n",
    "    cfg, cfg.DATASETS.TEST[0], mapper=dataset_mapper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gt_dicts(batched_inputs, json_file):\n",
    "\n",
    "    if os.path.exists(json_file):\n",
    "        with open(json_file, 'r') as handle:\n",
    "            json_dict = json.load(handle)\n",
    "    else:\n",
    "        json_dict = {\"images\": [],  \"annotations\": [], \"categories\": [{\"id\": 1, \"name\": \"box\"}, {\"id\": 2, \"name\": \"logo\"}]}\n",
    "\n",
    "    height, width = batched_inputs[0][\"instances\"].image_size\n",
    "    filename = '/'.join(batched_inputs[0]['file_name'].split('/')[-2:])\n",
    "    image_id = batched_inputs[0][\"image_id\"]\n",
    "    image = {\n",
    "        \"file_name\": filename,\n",
    "        \"height\": height,\n",
    "        \"width\": width,\n",
    "        \"id\": image_id,\n",
    "    }\n",
    "    json_dict[\"images\"].append(image)\n",
    "\n",
    "    ## get gt_box annotations\n",
    "    for i, b in enumerate(batched_inputs[0]['instances'].gt_boxes):\n",
    "        x1, y1, x2, y2 = int(b[0]), int(b[1]), int(b[2]), int(b[3])\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "\n",
    "        category_id = batched_inputs[0]['instances'].gt_classes[i].item()+1\n",
    "        id_annot = json_dict['annotations'][-1]['id']+1 if len(json_dict[\"annotations\"])!=0 else 0\n",
    "\n",
    "        ann = {\n",
    "            \"area\": width * height,\n",
    "            \"image_id\": image_id,\n",
    "            \"bbox\": [x1, y1, width, height],\n",
    "            \"category_id\": category_id,\n",
    "            \"id\": id_annot, # need to be continuous\n",
    "            \"iscrowd\": 0\n",
    "            }\n",
    "        json_dict[\"annotations\"].append(ann)\n",
    "\n",
    "    ## write to json file\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(json_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [00:28,  3.76it/s]/home/l/liny/anaconda3/envs/mypy37/lib/python3.7/site-packages/PIL/Image.py:2766: DecompressionBombWarning: Image size (112155664 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "1579it [07:55,  3.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in tqdm(enumerate(data_loader)):\n",
    "    save_gt_dicts(batch, 'data/benign_data/coco_perturbgt_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(749, 1333)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'data/benign_data/benign_database/demotywatory.pl/shot.png',\n",
       " 'height': 768,\n",
       " 'width': 1366,\n",
       " 'image_id': 0,\n",
       " 'image': tensor([[[23, 24, 24,  ..., 23, 23, 23],\n",
       "          [22, 23, 23,  ..., 22, 23, 24],\n",
       "          [21, 21, 21,  ..., 21, 23, 24],\n",
       "          ...,\n",
       "          [25, 25, 29,  ..., 30, 30, 26],\n",
       "          [25, 33, 26,  ..., 29, 27, 25],\n",
       "          [25, 31, 30,  ..., 32, 31, 30]],\n",
       " \n",
       "         [[22, 23, 23,  ..., 22, 22, 22],\n",
       "          [21, 22, 22,  ..., 21, 22, 23],\n",
       "          [20, 20, 20,  ..., 20, 22, 23],\n",
       "          ...,\n",
       "          [25, 25, 28,  ..., 29, 29, 26],\n",
       "          [25, 33, 26,  ..., 28, 27, 25],\n",
       "          [25, 30, 29,  ..., 31, 30, 29]],\n",
       " \n",
       "         [[31, 32, 32,  ..., 31, 31, 31],\n",
       "          [30, 31, 31,  ..., 30, 31, 32],\n",
       "          [29, 29, 29,  ..., 29, 31, 32],\n",
       "          ...,\n",
       "          [36, 36, 40,  ..., 41, 41, 37],\n",
       "          [36, 44, 37,  ..., 40, 38, 36],\n",
       "          [36, 42, 41,  ..., 43, 42, 41]]], dtype=torch.uint8),\n",
       " 'instances': Instances(num_instances=2, image_height=749, image_width=1333, fields=[gt_boxes: Boxes(tensor([[291.,   9., 514.,  37.],\n",
       "         [477., 124., 623., 141.]])), gt_classes: tensor([1, 0])])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = 'data/benign_data/benign_database/a-golf.net/shot.png'\n",
    "original_image = cv2.imread(original_image)\n",
    "with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "    # Apply pre-processing to image.\n",
    "#     if input_format == \"RGB\":\n",
    "#         # whether the model expects BGR inputs or RGB\n",
    "#         original_image = original_image[:, :, ::-1]\n",
    "    height, width = original_image.shape[:2]\n",
    "    image = aug.get_transform(original_image).apply_image(original_image)\n",
    "    image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "    inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "#     predictions = model([inputs])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instances': Instances(num_instances=9, image_height=1787, image_width=1366, fields=[pred_boxes: Boxes(tensor([[3.9013e+02, 3.6795e+02, 6.6060e+02, 4.0883e+02],\n",
       "         [3.9256e+02, 3.1160e+02, 6.6270e+02, 3.5157e+02],\n",
       "         [1.1783e+01, 6.4615e+00, 1.3082e+02, 4.8582e+01],\n",
       "         [1.8938e+02, 5.5595e+01, 3.8157e+02, 1.0328e+02],\n",
       "         [2.0437e+02, 2.7017e+02, 6.6726e+02, 3.0292e+02],\n",
       "         [1.1258e+02, 1.4594e+00, 4.8668e+02, 4.9569e+01],\n",
       "         [6.7648e+02, 1.4749e+03, 9.0259e+02, 1.5013e+03],\n",
       "         [4.3797e+02, 1.4726e+03, 6.6176e+02, 1.5030e+03],\n",
       "         [1.1484e+03, 4.5038e-01, 1.3660e+03, 5.0969e+01]], device='cuda:0')), scores: tensor([0.9912, 0.9910, 0.9066, 0.7832, 0.7655, 0.7319, 0.2090, 0.1220, 0.0753],\n",
       "        device='cuda:0'), pred_classes: tensor([0, 0, 1, 1, 0, 1, 0, 0, 1], device='cuda:0')])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess image\n",
    "images = model.preprocess_image([inputs])\n",
    "\n",
    "# Get features\n",
    "features = model.backbone(images.tensor)\n",
    "\n",
    "# Get bounding box proposals\n",
    "proposals, _ = model.proposal_generator(images, features, None)\n",
    "proposal_boxes = proposals[0].proposal_boxes\n",
    "\n",
    "def _get_roi_prediction(model, features, proposal_boxes):\n",
    "    # Get proposal boxes' classification scores\n",
    "    roi_heads = model.roi_heads\n",
    "    features = [features[f] for f in roi_heads.box_in_features]\n",
    "    box_features = roi_heads.box_pooler(features, [proposal_boxes])\n",
    "\n",
    "    box_features = roi_heads.box_head(box_features)\n",
    "    logits, proposal_deltas = roi_heads.box_predictor(box_features)\n",
    "    del box_features\n",
    "    \n",
    "    return logits, proposal_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "detectron2.structures.image_list.ImageList"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1047, 800])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, proposal_delta = _get_roi_prediction(model, features, proposal_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startind = torch.argmax(logits, dim=1)\n",
    "torch.nn.functional.one_hot(startind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 88, 1: 100, 2: 2812})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(startind.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "detectron2.structures.boxes.Boxes"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boxes(proposal_delta[:, :4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Boxes(tensor([[226.4303, 183.2361, 387.0454, 205.8438],\n",
       "        [228.3489, 215.5953, 385.8726, 239.3732],\n",
       "        [  6.9006,   5.5424,  78.0141,  28.2409],\n",
       "        ...,\n",
       "        [175.8883, 169.7452, 392.4770, 519.8912],\n",
       "        [440.4531, 433.6590, 662.4514, 748.8353],\n",
       "        [269.0080, 596.0848, 537.9231, 953.0919]], device='cuda:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposal_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(12.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aaec6e654ffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0monehot_tile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mregbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposal_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monehot_tile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mregbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mregbox\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "onehot_tile = onehot.repeat_interleave(4, dim=1)\n",
    "\n",
    "regbox = torch.mul(proposal_delta, onehot_tile)\n",
    "regbox = regbox[regbox!=0].view(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1108,  0.1453,  0.3384,  0.2832],\n",
       "        [-0.0285, -0.0651,  0.3444,  0.2503],\n",
       "        [-0.1576,  0.1342,  0.4125,  0.3486],\n",
       "        ...,\n",
       "        [ 0.5305, -0.6265, -1.1865,  0.0277],\n",
       "        [ 0.9470,  0.0714, -1.2006,  0.1700],\n",
       "        [-0.3846, -1.1510, -0.8399, -0.0300]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 4])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "regbox = regbox.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1108,  0.1453,  0.3384,  0.2832],\n",
       "        [-0.0285, -0.0651,  0.3444,  0.2503],\n",
       "        [-0.1576,  0.1342,  0.4125,  0.3486],\n",
       "        ...,\n",
       "        [ 0.5305, -0.6265, -1.1865,  0.0277],\n",
       "        [ 0.9470,  0.0714, -1.2006,  0.1700],\n",
       "        [-0.3846, -1.1510, -0.8399, -0.0300]], device='cuda:0')"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.normal(0, 0.1, size=inputs['image'].shape)\n",
    "\n",
    "inputs_noise = inputs.copy()\n",
    "\n",
    "inputs_noise['image'] = inputs['image'] + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess image\n",
    "images = model.preprocess_image([inputs_noise])\n",
    "\n",
    "# Record gradients for image\n",
    "images.tensor.requires_grad = True\n",
    "\n",
    "# Get features\n",
    "features = model.backbone(images.tensor)\n",
    "\n",
    "# Get bounding box proposals\n",
    "proposals, _ = model.proposal_generator(images, features, None)\n",
    "proposal_boxes = proposals[0].proposal_boxes\n",
    "\n",
    "# Get classification logits\n",
    "predictions = _get_roi_prediction(model, features, proposal_boxes)\n",
    "\n",
    "logits, proposal_deltas = predictions\n",
    "\n",
    "scores = model.roi_heads.box_predictor.predict_probs(\n",
    "    predictions, proposals\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "startind = torch.argmax(scores, dim=1)-1\n",
    "onehot = make_one_hot(startind, 2)\n",
    "onehot_tile = onehot.repeat_interleave(4, dim=1)\n",
    "\n",
    "proposal_deltas = torch.mul(proposal_deltas, onehot_tile)\n",
    "proposal_deltas = proposal_deltas[proposal_deltas!=0].view(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1111,  0.1455,  0.3384,  0.2839],\n",
       "        [-0.0276, -0.0647,  0.3481,  0.2524],\n",
       "        [-0.1579,  0.1343,  0.4133,  0.3497],\n",
       "        ...,\n",
       "        [-0.5342,  1.0924,  0.0970,  1.0639],\n",
       "        [-0.5792,  0.1298,  0.8606, -1.1056],\n",
       "        [-0.8079,  0.3824, -0.2702,  0.1062]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposal_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(torch.tensor([[1, 2, 1, 1]]), num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME Make this more efficient / vectorized?\n",
    "adv_labels = torch.zeros_like(labels)\n",
    "for i in range(len(labels)):\n",
    "    # Include background class: self.n_classes\n",
    "    incorrect_labels = [l for l in range(3) if l != labels[i]]\n",
    "    adv_labels[i] = random.choice(incorrect_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
